#!/usr/bin/env python
# coding: utf-8

# # <center>Data Science Project</center>
#                                                                            
#    # <center>Decision Trees</center>               
# 
#  # <center>By Will Potter</center>

# # Data Science Project Steps
# 
# 1. Define the Research problem and questions.
# 2. Analyze data by descriptive statistics and graphical visualization.
# 3. Prepare data by using relevant preprocessing transformations, data cleaning, 
#     data standardization,deaing with null and outlier values. Divide data into test and training set.
# 4. Fit the train data. 
# 5. Predict the test data.
# 6. Evaluate the first algorithm and its model performance.
# 7. Evaluate the current algorithm and variety of algorithms by creating test harness for diverse
#     algorithms in conjuction with resampling techniques like cross validation, variable importance.
#     bootstrapping. Improve Result by playing with hyperparameters and innovative methods like ensembles.
# 8. Choose the best model and present the results. 

# ## 1. Define the Research problem and questions
# 
# **Research Questions:**  
# 
#     1. Which characteristics of songs would be best to choose when creating a Decision Tree to model        how a song is classified in terms of Mood.
#     2. Is the prediction enhanced when using Random Forrest or Gradient Boosting Classifications  
# 

# ## 2. Analyze data by descriptive statistics and graphical visualization.

# In[1]:


# Import the following packages

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate
from sklearn.metrics import classification_report, confusion_matrix


# In[2]:


import warnings
warnings.filterwarnings('ignore')

dataMoodsData = pd.read_csv('heart_2020_cleaned.csv', nrows = 50)


#Cleaning Dataset, Dropping unused columns
dataMoodsData=dataMoodsData.drop('Asthma',axis=1)
dataMoodsData=dataMoodsData.drop('PhysicalHealth',axis=1)
dataMoodsData=dataMoodsData.drop('MentalHealth',axis=1)
dataMoodsData=dataMoodsData.drop('DiffWalking',axis=1)
dataMoodsData=dataMoodsData.drop('GenHealth',axis=1)
dataMoodsData=dataMoodsData.drop('KidneyDisease',axis=1)
dataMoodsData=dataMoodsData.drop('SkinCancer',axis=1)
# ## 3. Data Preparation  

# In[3]:

dataMoodsData['HeartDisease']=dataMoodsData['HeartDisease'].map({'No':0, 'Yes':1})
dataMoodsData['Smoking']=dataMoodsData['Smoking'].map({'No':0 ,'Yes':1})
dataMoodsData['Stroke']=dataMoodsData['Stroke'].map({'No':0 ,'Yes':1})
dataMoodsData['Sex']=dataMoodsData['Sex'].map({'Female':0 ,'Male':1})
dataMoodsData['AgeCategory']=dataMoodsData['AgeCategory'].map({'80 or older':0 ,'75-79':1, '70-74':2, '65-69':3, '60-64':4, '55-59':5, '50-54':6, '45-49':7, '40-44':8, '35-39':9, '30-34':10, '25-29':11, '18-24':12})
dataMoodsData['Race']=dataMoodsData['Race'].map({'White':0 ,'Black':1,'Asian':2,'American Indian/Alaskan Native':3,'Hispanic':4,'Other':5})
dataMoodsData['Diabetic']=dataMoodsData['Diabetic'].map({'No':0 ,'Yes':1,'No, borderline diabetes':2,'Yes (during pregnancy)':3})
dataMoodsData['PhysicalActivity']=dataMoodsData['PhysicalActivity'].map({'No':0 ,'Yes':1})
dataMoodsData['AlcoholDrinking']=dataMoodsData['AlcoholDrinking'].map({'No':0 ,'Yes':1})

#Normalizing Data
column = 'AgeCategory'
dataMoodsData[column] = (dataMoodsData[column] - dataMoodsData[column].min()) / (dataMoodsData[column].max() - dataMoodsData[column].min())

column = 'Race'
dataMoodsData[column] = (dataMoodsData[column] - dataMoodsData[column].min()) / (dataMoodsData[column].max() - dataMoodsData[column].min())

column = 'Diabetic'
dataMoodsData[column] = (dataMoodsData[column] - dataMoodsData[column].min()) / (dataMoodsData[column].max() - dataMoodsData[column].min())

column = 'BMI'
dataMoodsData[column] = (dataMoodsData[column] - dataMoodsData[column].min()) / (dataMoodsData[column].max() - dataMoodsData[column].min())

column = 'SleepTime'
dataMoodsData[column] = (dataMoodsData[column] - dataMoodsData[column].min()) / (dataMoodsData[column].max() - dataMoodsData[column].min()) 

print(dataMoodsData.shape)
print(dataMoodsData.head())

# Divide data into predictor features vector and the label of the trget variable 'Risk'

from sklearn.model_selection import train_test_split

#Splitting dataset by target and feature
dSet_target = dataMoodsData[["HeartDisease"]]
#print(dSet_target.to_string())
dSet_features = dataMoodsData[["BMI", "Smoking", "Stroke", "Sex", "AgeCategory", "Race", "Diabetic", "PhysicalActivity", "SleepTime", "AlcoholDrinking"]]
#print(dSet_features.to_string())

#Getting data ready for testing
X_train, X_test, y_train, y_test = train_test_split(dSet_features, dSet_target, test_size=0.3)


# Dividing data into two subsets :train and test set.Training set trains the model

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

# Data normalization  

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


# ## 4. Train Data and 5. Testing the data using Decision Tree  6. Evaluate

# In[4]:


# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html

from sklearn.tree import DecisionTreeClassifier

# Creating an instance of the Decision Tree
clf = DecisionTreeClassifier(random_state=0)
# Fiting the training data to the model
clf.fit(X_train, y_train)
# Printing the training accuracy with 3 significant figure accuracy
print("Accuracy on training set: {:.3f}".format(clf.score(X_train, y_train)))
# Printing the testing accuracy with 3 significant figure accuracy
print("Accuracy on test set: {:.3f}".format(clf.score(X_test, y_test)))


# In[5]:


from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier 
from sklearn import tree


clf = DecisionTreeClassifier(random_state=1234)
model = clf.fit(X_train, y_train)

text_representation = tree.export_text(clf)
print(text_representation)


# In[6]:


fig = plt.figure(figsize=(25,20))
features = ["BMI", "Smoking", "Stroke", "Sex", "AgeCategory", "Race", "Diabetic", "PhysicalActivity", "SleepTime", "AlcoholDrinking"]
tree.plot_tree(clf, filled = True, fontsize = 16, feature_names = features)
#fig.savefig("C:\\check\\decistion_tree.png")
plt.show()


# In[7]:


fig = plt.figure(figsize=(25,20))
tree.plot_tree(clf, filled = True, fontsize = 10, class_names = str(y))
#fig.savefig("C:\\check\\decistion_treefull.png")


# In[8]:


# Improving the Decision Tree accuracy by increasing the depth.
tree = DecisionTreeClassifier(max_depth=4, random_state=0)
tree.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(tree.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(tree.score(X_test, y_test)))


# In[9]:


print("Feature importances:")
print(tree.feature_importances_)


# ## 7. Feature Importance and techniques to improve performance

# In[10]:


# Creating a function to generate feature importance and plot it.

def plot_feature_importances_dataMoods(model):
# extractibg the number of predictive feature variables(columns)
    n_features = X.shape[1]
    print(n_features)
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), X)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)
    
# Calling the function to extract feature importance and passing the model named tree (constructed earlier) into it.

plot_feature_importances_dataMoods(tree)


# ## 8. Random Forest 

# In[11]:


# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_moons

# StratifiedKFold is a variation of k-fold which returns stratified folds: each set 
# contains approximately the same percentage of samples of each target class as the complete set.

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,
                                                    random_state=42)
# number of trees =100, maximum depth = 5
forest = RandomForestClassifier(max_depth=5,n_estimators=100, random_state=2)
forest.fit(X_train, y_train)
print("Accuracy on training set: {:.3f}".format(forest.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, y_test)))


# In[12]:


forest = RandomForestClassifier(n_estimators=100, random_state=0)
forest.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, y_test)))

# Calling the function to extract feature importance and passing the model named forest (constructed earlier) into it.
plot_feature_importances_dataMoods(forest)


# In[13]:


from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier 
from sklearn import tree


forest = DecisionTreeClassifier(random_state=1234)
model = forest.fit(X, y)
fig = plt.figure(figsize=(25,20))
tree.plot_tree(clf, max_depth = 2, filled = True, fontsize = 10, class_names = str(y))
plt.show()
#fig.savefig("C:\\check\\decistion_treeForest.png")


# ## 7. Boosting

# In[14]:


# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html

from sklearn.ensemble import GradientBoostingClassifier 

# Creating an instance of the Gradient Boosting Classifier and varying parameters.
gbrt = GradientBoostingClassifier(n_estimators=200,max_depth=3,learning_rate=0.03,random_state=0)
gbrt.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))


# In[15]:


gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)
gbrt.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))


# In[16]:


gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)
gbrt.fit(X_train, y_train)

print("Accuracy on training set: {:.3f}".format(gbrt.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(gbrt.score(X_test, y_test)))


# In[17]:


# Generating variable impotance

gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)
gbrt.fit(X_train, y_train)

plot_feature_importances_dataMoods(gbrt)


# In[18]:


from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier 
from sklearn import tree


gbrt = DecisionTreeClassifier(random_state=1234)
model = gbrt.fit(X, y)
fig = plt.figure(figsize=(25,20))
tree.plot_tree(clf, max_depth = 2, filled = True, fontsize = 10, class_names = str(y))
#fig.savefig("C:\\check\\decistion_treeGBRT.png")
plt.show()
